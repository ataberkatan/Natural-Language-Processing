{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Analysis of Two Congress Speeches\n",
    "### By calculating the Cosine Similarity between the TF-IDF (term frequency-inverse document frequency) and Count Vectors of texts with n-grams\n",
    "Using two speeches of my choosing,<br />\n",
    "preparing them for analysis (removing punctuation, stop-words),<br />\n",
    "using Bag of Words and n-grams in addition to tf-idf to find the cosine similarity between them.<br />\n",
    "Discussing my findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n%pip install -U scikit-learn\\n%pip install nltk \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "%pip install -U scikit-learn\n",
    "%pip install nltk \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nnltk.download('punkt')\\nnltk.download('wordnet')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import nltk delete if not necessary\n",
    "# import string\n",
    "# import numpy as np delete if not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1 head:\n",
      " <DOC>\n",
      "<DOCNO>105-moseleybraun-il-1-19981009</DOCNO>\n",
      "<TEXT>\n",
      " Ms. MOSELEYBRAUN. Mr. President, I want to note my disappointment that the permanent relief for Haitian refugees that I and many others in t \n",
      "\n",
      "text2 head:\n",
      " <DOC>\n",
      "<DOCNO>105-reid-nv-1-19981020</DOCNO>\n",
      "<TEXT>\n",
      " Mr. REID. Mr. President, I rise today to call attention to the outstanding achievements of a Nevadan who has dedicated himself to helping individual\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "text1_url = \"https://raw.githubusercontent.com/ariedamuco/ML-for-NLP/main/Inputs/105-extracted-date/105-moseleybraun-il.txt\"\n",
    "text2_url = \"https://raw.githubusercontent.com/ariedamuco/ML-for-NLP/main/Inputs/105-extracted-date/105-reid-nv.txt\"\n",
    "\n",
    "text1_get, text2_get = requests.get(text1_url), requests.get(text2_url)\n",
    "text1, text2 = text1_get.text, text2_get.text\n",
    "\n",
    "print(\"text1 head:\\n\",text1[0:200],\"\\n\\ntext2 head:\\n\",text2[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Loading the chosen speech texts:\\ntext1 = open(r\"Congress SpeechesE-moseleybraun-il.txt\").read()\\ntext2 = open(r\"Congress SpeechesE-reid-nv.txt\").read()\\n\\n# Printing the first 200 characters:\\nprint(\"text1 head:\\n\",text1[0:200],\"\\n\\ntext2 head:\\n\",text2[0:200])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "# Loading the chosen speech texts:\n",
    "text1 = open(r\"Congress Speeches\\105-moseleybraun-il.txt\").read()\n",
    "text2 = open(r\"Congress Speeches\\105-reid-nv.txt\").read()\n",
    "\n",
    "# Printing the first 200 characters:\n",
    "print(\"text1 head:\\n\",text1[0:200],\"\\n\\ntext2 head:\\n\",text2[0:200])\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing punctuation and stop-words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    # Deleting non-word characters by replacing them with blank (' '):\n",
    "    text= re.sub(r'\\W',' ', text)\n",
    "    # Tokenizing the string text into word substrings, writing them to a list (.lower() makes all characters lower case):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Removing English stopwords from the list:\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # Keeping words with at least 3 characters in the list:\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    # Joining the tokens -substrings- in the list back together with blank (' ') between them:\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun president want note disappointment permanent relief haitian refugees many others body worked make law dropped treasury appropriations conference report effort began last year debate appropriations bill included language granted certain central americans access suspension deportation procedure haitians granted access may recall supported granting relief affected class central americans along several colleagues senate house fought vigorously additional provisions haitian refugees although unsuccessful effort later introduced 1504 haitian immigrations fairness act 1997 legislation would provide haitian refugees permanent residency status course'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the pre-processing function with the first 1000 characters of the first text:\n",
    "text1_head_tokenized = text_preprocessor(text1[:1000])\n",
    "text1_head_tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming the words in the tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    # Creating a stemmer instance which uses Porter Stemming Algorithm:\n",
    "    stemmer = PorterStemmer()\n",
    "    # Tokenizing the text into words, stemming them:\n",
    "    stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "    # Joining the word stems back and returning:\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Some alternatives to Porter in NLTK are Snowball (in English) and Lancaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun presid want note disappoint perman relief haitian refuge mani other bodi work make law drop treasuri appropri confer report effort began last year debat appropri bill includ languag grant certain central american access suspens deport procedur haitian grant access may recal support grant relief affect class central american along sever colleagu senat hous fought vigor addit provis haitian refuge although unsuccess effort later introduc 1504 haitian immigr fair act 1997 legisl would provid haitian refuge perman resid statu cours'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the stemmer function:\n",
    "text1_stemmed = stem_words(text1_head_tokenized)\n",
    "text1_stemmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatizing the words in the tokenized and stemmed text\n",
    "[Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    # Creating a lemmatizer instance:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Applying the lemmatizer word by word:\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "    # Joining the words back and returning:\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun presid want note disappoint perman relief haitian refuge mani other bodi work make law drop treasuri appropri confer report effort began last year debat appropri bill includ languag grant certain central american access suspens deport procedur haitian grant access may recal support grant relief affect class central american along sever colleagu senat hous fought vigor addit provis haitian refuge although unsuccess effort later introduc 1504 haitian immigr fair act 1997 legisl would provid haitian refuge perman resid statu cours'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the lemmatizer function:\n",
    "text1_lemmatized = lemmatize_words(text1_stemmed)\n",
    "text1_lemmatized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "Now that all the pre-processing functions are tested and working, we can apply the functions to full bodies of both texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(text):\n",
    "    step1 = text_preprocessor(text)\n",
    "    step2 = stem_words(step1)\n",
    "    step3 = lemmatize_words(step2)\n",
    "    output = step3\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_processed, text2_processed = text_processor(text1), text_processor(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun presid want note disappoint perman relief haitian refuge mani other bodi work make law drop treasuri appropri confer report effort began last year debat appropri bill includ languag grant certain central american access suspens deport procedur haitian grant access may recal support grant relief affect class central american along sever colleagu senat hous fought vigor addit provis haitian refuge although unsuccess effort later introduc '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_processed[:500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We represent the processed bodies of text as vectors to analyze them. We use both TF-IDF and Bag-of-Words (Count) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Calling an instance of TF-IDF Vectorizer with default arguments:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calling an instance of Count Vectorizer with default arguments:\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the bodies of texts and putting them together in a matrix:\n",
    "corpus_tfidf = tfidf_vectorizer.fit_transform([text1_processed, text2_processed])\n",
    "corpus_count = count_vectorizer.fit_transform([text1_processed, text2_processed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transforming the corpus matrix to a dataframe with feature names (words) as index:\n",
    "corpus_tfidf_matrix = pd.DataFrame(corpus_tfidf.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "corpus_count_matrix = pd.DataFrame(corpus_count.toarray().transpose(), \n",
    "                             index=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Renaming the columns with the names of the senators who gave the speeches:\n",
    "corpus_tfidf_matrix = corpus_tfidf_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "\n",
    "corpus_count_matrix = corpus_count_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>192</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>060</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>063</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>083</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>097</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombi</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Moseley-Braun  Reid\n",
       "000               192   187\n",
       "060                 0     1\n",
       "063                 0     1\n",
       "083                 0     1\n",
       "097                 1     0\n",
       "...               ...   ...\n",
       "zero               13     3\n",
       "zest                1     0\n",
       "zombi               1     0\n",
       "zone               15     1\n",
       "zoster              0     1\n",
       "\n",
       "[8494 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.047383</td>\n",
       "      <td>0.041278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>060</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>063</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>083</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>097</th>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombi</th>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Moseley-Braun      Reid\n",
       "000          0.047383  0.041278\n",
       "060          0.000000  0.000310\n",
       "063          0.000000  0.000310\n",
       "083          0.000000  0.000310\n",
       "097          0.000347  0.000000\n",
       "...               ...       ...\n",
       "zero         0.003208  0.000662\n",
       "zest         0.000347  0.000000\n",
       "zombi        0.000347  0.000000\n",
       "zone         0.003702  0.000221\n",
       "zoster       0.000000  0.000310\n",
       "\n",
       "[8494 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculating the cosine similarity between to the vectorized texts:\n",
    "tfidf_result = cosine_similarity(corpus_tfidf_matrix[corpus_tfidf_matrix.columns[0]].values.reshape(1, -1), \n",
    "                  corpus_tfidf_matrix[corpus_tfidf_matrix.columns[1]].values.reshape(1, -1))\n",
    "\n",
    "bow_result = cosine_similarity(corpus_count_matrix[corpus_count_matrix.columns[0]].values.reshape(1, -1), \n",
    "                  corpus_count_matrix[corpus_count_matrix.columns[1]].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity rate with TF-IDF: [[0.69861398]] \n",
      "Similarity rate with Bag-of-Words: [[0.7534336]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity rate with TF-IDF:\", tfidf_result, \n",
    "      \"\\nSimilarity rate with Bag-of-Words:\", bow_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us see how the results change if we look at 2-grams and 3-grams cumulatively in addition to single words.\n",
    "\n",
    "We need to modify the vectorizers in order to achieve this. Previously, we used the vectorizers with default parameters. This means that they only looked at single words instead of groups of two or three consecutive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling an instance of TF-IDF Vectorizer with 1 to 2 grams:\n",
    "tfidf_vectorizer_12 = TfidfVectorizer(ngram_range = (3,3))\n",
    "\n",
    "# Calling an instance of Count Vectorizer with 1 to 2 grams:\n",
    "count_vectorizer_12 = CountVectorizer(ngram_range = (3,3))\n",
    "\n",
    "# I name the vectorizers with the \"_12\" suffix, indicating the ngram_range parameter values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we call the vectorizers to look at 1 and 2-grams together. We could also look at 2-grams only instead by setting the ngram_range parameter to (2,2) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the bodies of texts and putting them together in a matrix:\n",
    "corpus_tfidf_12 = tfidf_vectorizer_12.fit_transform([text1_processed, text2_processed])\n",
    "corpus_count_12 = count_vectorizer_12.fit_transform([text1_processed, text2_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the corpus matrix to a dataframe with feature names (words) as index:\n",
    "corpus_tfidf_matrix_12 = pd.DataFrame(corpus_tfidf_12.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer_12.get_feature_names_out())\n",
    "\n",
    "corpus_count_matrix_12 = pd.DataFrame(corpus_count_12.toarray().transpose(), \n",
    "                             index=count_vectorizer_12.get_feature_names_out())\n",
    "\n",
    "# Renaming the columns with the names of the senators who gave the speeches:\n",
    "corpus_tfidf_matrix_12 = corpus_tfidf_matrix_12.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "\n",
    "corpus_count_matrix_12 = corpus_count_matrix_12.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000 000 benefit</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 per</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 portion</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 taxpay</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 week</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone initi cornerston</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone new enterpris</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone peac democraci</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone urban rural</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster malaria hepat</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154370 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Moseley-Braun  Reid\n",
       "000 000 benefit                    1     0\n",
       "000 000 per                        1     0\n",
       "000 000 portion                    0     1\n",
       "000 000 taxpay                     1     0\n",
       "000 000 week                       0     2\n",
       "...                              ...   ...\n",
       "zone initi cornerston              1     0\n",
       "zone new enterpris                 1     0\n",
       "zone peac democraci                1     0\n",
       "zone urban rural                   1     0\n",
       "zoster malaria hepat               0     1\n",
       "\n",
       "[154370 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_count_matrix_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000 000 benefit</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 per</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 portion</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 taxpay</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000 week</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone initi cornerston</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone new enterpris</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone peac democraci</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone urban rural</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster malaria hepat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154370 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Moseley-Braun      Reid\n",
       "000 000 benefit             0.001185  0.000000\n",
       "000 000 per                 0.001185  0.000000\n",
       "000 000 portion             0.000000  0.000735\n",
       "000 000 taxpay              0.001185  0.000000\n",
       "000 000 week                0.000000  0.001470\n",
       "...                              ...       ...\n",
       "zone initi cornerston       0.001185  0.000000\n",
       "zone new enterpris          0.001185  0.000000\n",
       "zone peac democraci         0.001185  0.000000\n",
       "zone urban rural            0.001185  0.000000\n",
       "zoster malaria hepat        0.000000  0.000735\n",
       "\n",
       "[154370 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf_matrix_12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function for cosine similarity to make life easier in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity rate with TF-IDF: [[0.69861398]] \n",
      "Similarity rate with Bag-of-Words: [[0.7534336]] \n",
      "Similarity rate with TF-IDF, 1 to 2-grams: [[0.35161355]] \n",
      "Similarity rate with Bag-of-Words, 1 to 2-grams: [[0.5146463]]\n"
     ]
    }
   ],
   "source": [
    "def cosine_result(matrix):\n",
    "    result = cosine_similarity(matrix[matrix.columns[0]].values.reshape(1, -1), \n",
    "                               matrix[matrix.columns[1]].values.reshape(1, -1))\n",
    "    return result\n",
    "\n",
    "tfidf_result_12 = cosine_result(corpus_tfidf_matrix_12)\n",
    "bow_result_12 = cosine_result(corpus_count_matrix_12)\n",
    "\n",
    "print(\"Similarity rate with TF-IDF:\", tfidf_result,\n",
    "      \"\\nSimilarity rate with Bag-of-Words:\", bow_result, \n",
    "      \"\\nSimilarity rate with TF-IDF, 1 to 2-grams:\", tfidf_result_12, \n",
    "      \"\\nSimilarity rate with Bag-of-Words, 1 to 2-grams:\", bow_result_12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define another function which takes two processed texts, ngram_range parameters and vectorizer as input and returns cosine similarity between the two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_pipeline(vectorizer, txt1, txt2, ngram_range = (1,1)):\n",
    "    \n",
    "    # Allowing the option to use Tfidf and Count vectorizers:\n",
    "    if vectorizer == \"Count\":\n",
    "        \n",
    "        # Calling the vectorizer with desired ngram_range values, (1,1) applies if not specified:\n",
    "        count_vectorizer = CountVectorizer(ngram_range = ngram_range)\n",
    "        \n",
    "        corpus_count = count_vectorizer.fit_transform([txt1, txt2])\n",
    "\n",
    "        # Loading vectorized texts into a matrix:\n",
    "        corpus_count_matrix = pd.DataFrame(corpus_count.toarray().transpose(), \n",
    "                             index=count_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Renaming the columns:\n",
    "        corpus_count_matrix = corpus_count_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "        \n",
    "        # Defining the output:\n",
    "        output = cosine_result(corpus_count_matrix)\n",
    "    \n",
    "    elif vectorizer == \"Tfidf\":\n",
    "        # Calling the vectorizer with desired ngram_range values, (1,1) applies if not specified:\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range = ngram_range)\n",
    "        \n",
    "        corpus_tfidf = tfidf_vectorizer.fit_transform([txt1, txt2])\n",
    "\n",
    "        # Loading vectorized texts into a matrix:\n",
    "        corpus_tfidf_matrix = pd.DataFrame(corpus_tfidf.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Renaming the columns:\n",
    "        corpus_tfidf_matrix = corpus_tfidf_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "        \n",
    "        # Defining the output:\n",
    "        output = cosine_result(corpus_tfidf_matrix)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46161153]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function:\n",
    "similarity_pipeline(\"Tfidf\", text1_processed, text2_processed, ngram_range = (2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check results comparatively, produced from different vectorizers and specifications of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Ngram Range</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.698614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.658574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.626878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>0.461612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.351614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.753434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.729314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.710265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>0.610052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.514646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Vectorizer Ngram Range  Similarity Score\n",
       "0         Tfidf      (1, 1)          0.698614\n",
       "1         Tfidf      (1, 2)          0.658574\n",
       "2         Tfidf      (1, 3)          0.626878\n",
       "3         Tfidf      (2, 2)          0.461612\n",
       "4         Tfidf      (3, 3)          0.351614\n",
       "5  Bag-of-Words      (1, 1)          0.753434\n",
       "6  Bag-of-Words      (1, 2)          0.729314\n",
       "7  Bag-of-Words      (1, 3)          0.710265\n",
       "8  Bag-of-Words      (2, 2)          0.610052\n",
       "9  Bag-of-Words      (3, 3)          0.514646"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_ranges = [(1,1), (1,2), (1,3), (2,2), (3,3)]\n",
    "vectorizers = [\"Tfidf\", \"Count\"]\n",
    "results = []\n",
    "for vec in vectorizers:\n",
    "    for ngram_range in ngram_ranges:\n",
    "        if vec == \"Tfidf\":\n",
    "            score = similarity_pipeline(vec, text1_processed, text2_processed, ngram_range=ngram_range)\n",
    "            results.append({\"Vectorizer\": vec, \"Ngram Range\": ngram_range, \"Similarity Score\": score[0][0]})\n",
    "        else:\n",
    "            score = similarity_pipeline(vec, text1_processed, text2_processed, ngram_range=ngram_range)\n",
    "            results.append({\"Vectorizer\": \"Bag-of-Words\", \"Ngram Range\": ngram_range, \"Similarity Score\": score[0][0]})\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "To qualitatively discuss the similarity between the two senators' speeches some background on who they are is needed.\n",
    "\n",
    "[**Moseley-Braun**](https://en.wikipedia.org/wiki/Carol_Moseley_Braun) was *the first African-American woman* elected to the U.S. Senate, the first African-American U.S. Senator from the **Democratic Party**, *the first woman to defeat an incumbent U.S. Senator in an election*, and the first female U.S. Senator from Illinois.\n",
    "\n",
    "[**Harry Mason Reid Jr.**](https://en.wikipedia.org/wiki/Harry_Reid) was an American lawyer and politician who served as a United States senator from Nevada from 1987 to 2017. He *led the Senate **Democratic** Caucus from 2005 to 2017* and was *the Senate Majority Leader from 2007 to 2015*.\n",
    "\n",
    "Although both senators were from the same party, the dissimilarity between them should most likely to be rooted in their backgrounds and identities they stand for.\n",
    "\n",
    "Moseley-Braun had been the first to set many milestones while Reid's election and re-ellection has arguably been in smoother conditions. Moseley-Braun is an African-American woman to be the first female U.S. Senator in her state while Reid is from an already well represented identity group - white and male.\n",
    "\n",
    "The similarity being above 50% might be due to the fact that they are from the same party but the present difference is, at least superficially, because they are vastly different character and from very different states.\n",
    "\n",
    "For a better grounded analysis, we can look at the similarity measures of multiple pairs of senator speeches from the same and different parties and employ a comparative perspective. This approach can reveal patterns more clearly as to what makes two speeches similar and what having similar speeches tells us about the characteristics of the senators in comparison. Furthermore, different methods of vectorizing speeches and different measures of similarity might give qualitatively different results.\n",
    "\n",
    "For instance, we found here a similarity of 65%. A good reference point would be the average level of similarity between senators of the two different parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd99e0ac365466ae650ff45d8775d59e259e5553338a3c6426640fcc04918b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
