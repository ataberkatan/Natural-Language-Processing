{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Analysis of Congress Speeches\n",
    "### By calculating the Cosine Similarities and Manhattan Distances between the TF-IDF (term frequency-inverse document frequency) and Count Vectors of texts with n-grams\n",
    "Using *N* speeches of my choosing,<br />\n",
    "preparing them for analysis (removing punctuation, stop-words),<br />\n",
    "using Bag of Words and n-grams in addition to tf-idf to find the cosine similarity between them.<br />\n",
    "Discussing my findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n%pip install -U scikit-learn\\n%pip install nltk \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "%pip install -U scikit-learn\n",
    "%pip install nltk \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nnltk.download('punkt')\\nnltk.download('wordnet')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1 head:\n",
      " <DOC>\n",
      "<DOCNO>105-moseleybraun-il-1-19981009</DOCNO>\n",
      "<TEXT>\n",
      " Ms. MOSELEYBRAUN. Mr. President, I want to note my disappointment that the permanent relief for Haitian refugees that I and many others in t \n",
      "\n",
      "text2 head:\n",
      " <DOC>\n",
      "<DOCNO>105-reid-nv-1-19981020</DOCNO>\n",
      "<TEXT>\n",
      " Mr. REID. Mr. President, I rise today to call attention to the outstanding achievements of a Nevadan who has dedicated himself to helping individual\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "text1_url = \"https://raw.githubusercontent.com/ariedamuco/ML-for-NLP/main/Inputs/105-extracted-date/105-moseleybraun-il.txt\"\n",
    "text2_url = \"https://raw.githubusercontent.com/ariedamuco/ML-for-NLP/main/Inputs/105-extracted-date/105-reid-nv.txt\"\n",
    "\n",
    "text1_get, text2_get = requests.get(text1_url), requests.get(text2_url)\n",
    "text1, text2 = text1_get.text, text2_get.text\n",
    "\n",
    "print(\"text1 head:\\n\",text1[0:200],\"\\n\\ntext2 head:\\n\",text2[0:200])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing punctuation and stop-words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    # Deleting non-word characters by replacing them with blank (' '):\n",
    "    text= re.sub(r'\\W',' ', text)\n",
    "    # Tokenizing the string text into word substrings, writing them to a list (.lower() makes all characters lower case):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Removing English stopwords from the list:\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # Keeping words with at least 3 characters in the list:\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    # Joining the tokens -substrings- in the list back together with blank (' ') between them:\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun president want note disappointment permanent relief haitian refugees many others body worked make law dropped treasury appropriations conference report effort began last year debate appropriations bill included language granted certain central americans access suspension deportation procedure haitians granted access may recall supported granting relief affected class central americans along several colleagues senate house fought vigorously additional provisions haitian refugees although unsuccessful effort later introduced 1504 haitian immigrations fairness act 1997 legislation would provide haitian refugees permanent residency status course'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the pre-processing function with the first 1000 characters of the first text:\n",
    "text1_head_tokenized = text_preprocessor(text1[:1000])\n",
    "text1_head_tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming the words in the tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    # Creating a stemmer instance which uses Porter Stemming Algorithm:\n",
    "    stemmer = PorterStemmer()\n",
    "    # Tokenizing the text into words, stemming them:\n",
    "    stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "    # Joining the word stems back and returning:\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Some alternatives to Porter in NLTK are Snowball (in English) and Lancaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun presid want note disappoint perman relief haitian refuge mani other bodi work make law drop treasuri appropri confer report effort began last year debat appropri bill includ languag grant certain central american access suspens deport procedur haitian grant access may recal support grant relief affect class central american along sever colleagu senat hous fought vigor addit provis haitian refuge although unsuccess effort later introduc 1504 haitian immigr fair act 1997 legisl would provid haitian refuge perman resid statu cours'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the stemmer function:\n",
    "text1_stemmed = stem_words(text1_head_tokenized)\n",
    "text1_stemmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatizing the words in the tokenized and stemmed text\n",
    "[Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    # Creating a lemmatizer instance:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Applying the lemmatizer word by word:\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "    # Joining the words back and returning:\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun presid want note disappoint perman relief haitian refuge mani other bodi work make law drop treasuri appropri confer report effort began last year debat appropri bill includ languag grant certain central american access suspens deport procedur haitian grant access may recal support grant relief affect class central american along sever colleagu senat hous fought vigor addit provis haitian refuge although unsuccess effort later introduc 1504 haitian immigr fair act 1997 legisl would provid haitian refuge perman resid statu cours'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the lemmatizer function:\n",
    "text1_lemmatized = lemmatize_words(text1_stemmed)\n",
    "text1_lemmatized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "Now that all the pre-processing functions are tested and working, we can apply the functions to full bodies of both texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(text):\n",
    "    step1 = text_preprocessor(text)\n",
    "    step2 = stem_words(step1)\n",
    "    step3 = lemmatize_words(step2)\n",
    "    output = step3\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_processed, text2_processed = text_processor(text1), text_processor(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 moseleybraun 19981009 docno text moseleybraun presid want note disappoint perman relief haitian refuge mani other bodi work make law drop treasuri appropri confer report effort began last year debat appropri bill includ languag grant certain central american access suspens deport procedur haitian grant access may recal support grant relief affect class central american along sever colleagu senat hous fought vigor addit provis haitian refuge although unsuccess effort later introduc '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_processed[:500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We represent the processed bodies of text as vectors to analyze them. We use both TF-IDF and Bag-of-Words (Count) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Calling an instance of TF-IDF Vectorizer with default arguments:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calling an instance of Count Vectorizer with default arguments:\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the bodies of texts and putting them together in a matrix:\n",
    "corpus_tfidf = tfidf_vectorizer.fit_transform([text1_processed, text2_processed])\n",
    "corpus_count = count_vectorizer.fit_transform([text1_processed, text2_processed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transforming the corpus matrix to a dataframe with feature names (words) as index:\n",
    "corpus_tfidf_matrix = pd.DataFrame(corpus_tfidf.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "corpus_count_matrix = pd.DataFrame(corpus_count.toarray().transpose(), \n",
    "                             index=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Renaming the columns with the names of the senators who gave the speeches:\n",
    "corpus_tfidf_matrix = corpus_tfidf_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "\n",
    "corpus_count_matrix = corpus_count_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>192</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>060</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>063</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>083</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>097</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombi</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Moseley-Braun  Reid\n",
       "000               192   187\n",
       "060                 0     1\n",
       "063                 0     1\n",
       "083                 0     1\n",
       "097                 1     0\n",
       "...               ...   ...\n",
       "zero               13     3\n",
       "zest                1     0\n",
       "zombi               1     0\n",
       "zone               15     1\n",
       "zoster              0     1\n",
       "\n",
       "[8494 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.047383</td>\n",
       "      <td>0.041278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>060</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>063</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>083</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>097</th>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombi</th>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Moseley-Braun      Reid\n",
       "000          0.047383  0.041278\n",
       "060          0.000000  0.000310\n",
       "063          0.000000  0.000310\n",
       "083          0.000000  0.000310\n",
       "097          0.000347  0.000000\n",
       "...               ...       ...\n",
       "zero         0.003208  0.000662\n",
       "zest         0.000347  0.000000\n",
       "zombi        0.000347  0.000000\n",
       "zone         0.003702  0.000221\n",
       "zoster       0.000000  0.000310\n",
       "\n",
       "[8494 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculating the cosine similarity between to the vectorized texts:\n",
    "tfidf_result = cosine_similarity(corpus_tfidf_matrix[corpus_tfidf_matrix.columns[0]].values.reshape(1, -1), \n",
    "                  corpus_tfidf_matrix[corpus_tfidf_matrix.columns[1]].values.reshape(1, -1))\n",
    "\n",
    "bow_result = cosine_similarity(corpus_count_matrix[corpus_count_matrix.columns[0]].values.reshape(1, -1), \n",
    "                  corpus_count_matrix[corpus_count_matrix.columns[1]].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity rate with TF-IDF: [[0.69861398]] \n",
      "Similarity rate with Bag-of-Words: [[0.7534336]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity rate with TF-IDF:\", tfidf_result, \n",
    "      \"\\nSimilarity rate with Bag-of-Words:\", bow_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us see how the results change if we look at 2-grams and 3-grams cumulatively in addition to single words.\n",
    "\n",
    "We need to modify the vectorizers in order to achieve this. Previously, we used the vectorizers with default parameters. This means that they only looked at single words instead of groups of two or three consecutive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling an instance of TF-IDF Vectorizer with 1 to 2 grams:\n",
    "tfidf_vectorizer_12 = TfidfVectorizer(ngram_range = (1,2))\n",
    "\n",
    "# Calling an instance of Count Vectorizer with 1 to 2 grams:\n",
    "count_vectorizer_12 = CountVectorizer(ngram_range = (1,2))\n",
    "\n",
    "# I name the vectorizers with the \"_12\" suffix, indicating the ngram_range parameter values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we call the vectorizers to look at 1 and 2-grams together. We could also look at 2-grams only instead by setting the ngram_range parameter to (2,2) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the bodies of texts and putting them together in a matrix:\n",
    "corpus_tfidf_12 = tfidf_vectorizer_12.fit_transform([text1_processed, text2_processed])\n",
    "corpus_count_12 = count_vectorizer_12.fit_transform([text1_processed, text2_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the corpus matrix to a dataframe with feature names (words) as index:\n",
    "corpus_tfidf_matrix_12 = pd.DataFrame(corpus_tfidf_12.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer_12.get_feature_names_out())\n",
    "\n",
    "corpus_count_matrix_12 = pd.DataFrame(corpus_count_12.toarray().transpose(), \n",
    "                             index=count_vectorizer_12.get_feature_names_out())\n",
    "\n",
    "# Renaming the columns with the names of the senators who gave the speeches:\n",
    "corpus_tfidf_matrix_12 = corpus_tfidf_matrix_12.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "\n",
    "corpus_count_matrix_12 = corpus_count_matrix_12.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>192</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 170</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 1990</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 1995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone new</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone peac</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone urban</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster malaria</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120927 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Moseley-Braun  Reid\n",
       "000                       192   187\n",
       "000 000                     4     3\n",
       "000 170                     0     1\n",
       "000 1990                    1     0\n",
       "000 1995                    1     0\n",
       "...                       ...   ...\n",
       "zone new                    1     0\n",
       "zone peac                   1     0\n",
       "zone urban                  1     0\n",
       "zoster                      0     1\n",
       "zoster malaria              0     1\n",
       "\n",
       "[120927 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_count_matrix_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moseley-Braun</th>\n",
       "      <th>Reid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.044707</td>\n",
       "      <td>0.036991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 000</th>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.000593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 170</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 1990</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 1995</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone new</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone peac</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone urban</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoster malaria</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120927 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Moseley-Braun      Reid\n",
       "000                  0.044707  0.036991\n",
       "000 000              0.000931  0.000593\n",
       "000 170              0.000000  0.000278\n",
       "000 1990             0.000327  0.000000\n",
       "000 1995             0.000327  0.000000\n",
       "...                       ...       ...\n",
       "zone new             0.000327  0.000000\n",
       "zone peac            0.000327  0.000000\n",
       "zone urban           0.000327  0.000000\n",
       "zoster               0.000000  0.000278\n",
       "zoster malaria       0.000000  0.000278\n",
       "\n",
       "[120927 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf_matrix_12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define functions for similarity measures to make life easier in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity rate with TF-IDF: [[0.69861398]] \n",
      "Similarity rate with Bag-of-Words: [[0.7534336]] \n",
      "Similarity rate with TF-IDF, 1 to 2-grams: [[0.65857355]] \n",
      "Similarity rate with Bag-of-Words, 1 to 2-grams: [[0.72931358]]\n"
     ]
    }
   ],
   "source": [
    "def cosine_result(matrix):\n",
    "    result = cosine_similarity(matrix[matrix.columns[0]].values.reshape(1, -1), \n",
    "                               matrix[matrix.columns[1]].values.reshape(1, -1))\n",
    "    return result\n",
    "\n",
    "tfidf_result_12 = cosine_result(corpus_tfidf_matrix_12)\n",
    "bow_result_12 = cosine_result(corpus_count_matrix_12)\n",
    "\n",
    "print(\"Similarity rate with TF-IDF:\", tfidf_result,\n",
    "      \"\\nSimilarity rate with Bag-of-Words:\", bow_result, \n",
    "      \"\\nSimilarity rate with TF-IDF, 1 to 2-grams:\", tfidf_result_12, \n",
    "      \"\\nSimilarity rate with Bag-of-Words, 1 to 2-grams:\", bow_result_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "def manhattan_result(matrix):\n",
    "    result = manhattan_distances(matrix[matrix.columns[0]].values.reshape(1, -1), \n",
    "                               matrix[matrix.columns[1]].values.reshape(1, -1))\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define another function which takes two processed texts, ngram_range parameters and vectorizer as input and returns cosine similarity between the two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_pipeline(vectorizer, txt1, txt2, ngram_range = (1,1), similarity = \"Cosine\"):\n",
    "    \n",
    "    # Allowing the option to use Tfidf and Count vectorizers:\n",
    "    if vectorizer == \"Count\":\n",
    "        \n",
    "        # Calling the vectorizer with desired ngram_range values, (1,1) applies if not specified:\n",
    "        count_vectorizer = CountVectorizer(ngram_range = ngram_range)\n",
    "        \n",
    "        corpus_count = count_vectorizer.fit_transform([txt1, txt2])\n",
    "\n",
    "        # Loading vectorized texts into a matrix:\n",
    "        corpus_count_matrix = pd.DataFrame(corpus_count.toarray().transpose(), \n",
    "                             index=count_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Renaming the columns:\n",
    "        corpus_count_matrix = corpus_count_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "        \n",
    "        # Defining the output:\n",
    "        if similarity == \"Cosine\":\n",
    "            output = cosine_result(corpus_count_matrix)\n",
    "\n",
    "        elif similarity == \"Manhattan\":\n",
    "            output = manhattan_result(corpus_count_matrix)\n",
    "\n",
    "        else:\n",
    "            print(\"Please choose a valid parameter for similarity.\",\n",
    "                  \"\\nValid similarity measures are 'Cosine' and 'Manhattan'.\")\n",
    "    \n",
    "    elif vectorizer == \"Tfidf\":\n",
    "        # Calling the vectorizer with desired ngram_range values, (1,1) applies if not specified:\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range = ngram_range)\n",
    "        \n",
    "        corpus_tfidf = tfidf_vectorizer.fit_transform([txt1, txt2])\n",
    "\n",
    "        # Loading vectorized texts into a matrix:\n",
    "        corpus_tfidf_matrix = pd.DataFrame(corpus_tfidf.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Renaming the columns:\n",
    "        corpus_tfidf_matrix = corpus_tfidf_matrix.set_axis([\"Moseley-Braun\",\"Reid\"], \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)\n",
    "        \n",
    "        # Defining the output:\n",
    "        if similarity == \"Cosine\":\n",
    "            output = cosine_result(corpus_tfidf_matrix)\n",
    "\n",
    "        elif similarity == \"Manhattan\":\n",
    "            output = manhattan_result(corpus_tfidf_matrix)\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Please choose valid parameters for vectorizer and similarity measure.\",\n",
    "              \"\\nValid vectorizers are 'Count' and 'Tfidf'.\",\n",
    "              \"\\nValid similarity measures are 'Cosine' and 'Manhattan'.\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[122.85798728]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function:\n",
    "similarity_pipeline(\"Tfidf\", text1_processed, text2_processed, ngram_range = (2,2), similarity= \"Manhattan\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check results comparatively, produced from different vectorizers and specifications of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Similarity Measure</th>\n",
       "      <th>Ngram Range</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.698614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>16.875941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.658574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>61.226410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.626878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>108.874310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.610308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>155.732114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>0.461612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>122.857987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.351614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>169.456308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>0.358594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>213.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.753434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>64225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.729314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>217514.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.710265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>392813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.700176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>571048.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>0.610052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>153289.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>0.514646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>175299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>0.523280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>178235.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Vectorizer Similarity Measure Ngram Range  Similarity Score\n",
       "0          Tfidf             Cosine      (1, 1)          0.698614\n",
       "1          Tfidf          Manhattan      (1, 1)         16.875941\n",
       "2          Tfidf             Cosine      (1, 2)          0.658574\n",
       "3          Tfidf          Manhattan      (1, 2)         61.226410\n",
       "4          Tfidf             Cosine      (1, 3)          0.626878\n",
       "5          Tfidf          Manhattan      (1, 3)        108.874310\n",
       "6          Tfidf             Cosine      (1, 4)          0.610308\n",
       "7          Tfidf          Manhattan      (1, 4)        155.732114\n",
       "8          Tfidf             Cosine      (2, 2)          0.461612\n",
       "9          Tfidf          Manhattan      (2, 2)        122.857987\n",
       "10         Tfidf             Cosine      (3, 3)          0.351614\n",
       "11         Tfidf          Manhattan      (3, 3)        169.456308\n",
       "12         Tfidf             Cosine      (4, 4)          0.358594\n",
       "13         Tfidf          Manhattan      (4, 4)        213.775900\n",
       "14  Bag-of-Words             Cosine      (1, 1)          0.753434\n",
       "15  Bag-of-Words          Manhattan      (1, 1)      64225.000000\n",
       "16  Bag-of-Words             Cosine      (1, 2)          0.729314\n",
       "17  Bag-of-Words          Manhattan      (1, 2)     217514.000000\n",
       "18  Bag-of-Words             Cosine      (1, 3)          0.710265\n",
       "19  Bag-of-Words          Manhattan      (1, 3)     392813.000000\n",
       "20  Bag-of-Words             Cosine      (1, 4)          0.700176\n",
       "21  Bag-of-Words          Manhattan      (1, 4)     571048.000000\n",
       "22  Bag-of-Words             Cosine      (2, 2)          0.610052\n",
       "23  Bag-of-Words          Manhattan      (2, 2)     153289.000000\n",
       "24  Bag-of-Words             Cosine      (3, 3)          0.514646\n",
       "25  Bag-of-Words          Manhattan      (3, 3)     175299.000000\n",
       "26  Bag-of-Words             Cosine      (4, 4)          0.523280\n",
       "27  Bag-of-Words          Manhattan      (4, 4)     178235.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_ranges = [(1,1), (1,2), (1,3), (1,4), (2,2), (3,3), (4,4)]\n",
    "vectorizers = [\"Tfidf\", \"Count\"]\n",
    "similarity_measures = [\"Cosine\", \"Manhattan\"]\n",
    "results = []\n",
    "for vec in vectorizers:\n",
    "    for ngram_range in ngram_ranges:\n",
    "        for similarity_measure in similarity_measures:\n",
    "            if vec == \"Tfidf\":\n",
    "                if similarity_measure == \"Cosine\":\n",
    "                    score = similarity_pipeline(vec, text1_processed, text2_processed, ngram_range=ngram_range, similarity=\"Cosine\")\n",
    "                    results.append({\"Vectorizer\": vec, \"Similarity Measure\": similarity_measure, \"Ngram Range\": ngram_range, \"Similarity Score\": score[0][0]})\n",
    "                else:\n",
    "                    score = similarity_pipeline(vec, text1_processed, text2_processed, ngram_range=ngram_range, similarity=\"Manhattan\")\n",
    "                    results.append({\"Vectorizer\": vec, \"Similarity Measure\": \"Manhattan\", \"Ngram Range\": ngram_range, \"Similarity Score\": score[0][0]})\n",
    "            else:\n",
    "                if similarity_measure == \"Cosine\":\n",
    "                    score = similarity_pipeline(vec, text1_processed, text2_processed, ngram_range=ngram_range, similarity=\"Cosine\")\n",
    "                    results.append({\"Vectorizer\": \"Bag-of-Words\", \"Similarity Measure\": similarity_measure, \"Ngram Range\": ngram_range, \"Similarity Score\": score[0][0]})\n",
    "                else:\n",
    "                    score = similarity_pipeline(vec, text1_processed, text2_processed, ngram_range=ngram_range, similarity=\"Manhattan\")\n",
    "                    results.append({\"Vectorizer\": \"Bag-of-Words\", \"Similarity Measure\": \"Manhattan\", \"Ngram Range\": ngram_range, \"Similarity Score\": score[0][0]})\n",
    " \n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "To qualitatively discuss the similarity between the two senators' speeches some background on who they are is needed.\n",
    "\n",
    "[**Moseley-Braun**](https://en.wikipedia.org/wiki/Carol_Moseley_Braun) was *the first African-American woman* elected to the U.S. Senate, the first African-American U.S. Senator from the **Democratic Party**, *the first woman to defeat an incumbent U.S. Senator in an election*, and the first female U.S. Senator from Illinois.\n",
    "\n",
    "[**Harry Mason Reid Jr.**](https://en.wikipedia.org/wiki/Harry_Reid) was an American lawyer and politician who served as a United States senator from Nevada from 1987 to 2017. He *led the Senate **Democratic** Caucus from 2005 to 2017* and was *the Senate Majority Leader from 2007 to 2015*.\n",
    "\n",
    "Although both senators were from the same party, the dissimilarity between them should most likely to be rooted in their backgrounds and identities they stand for.\n",
    "\n",
    "Moseley-Braun had been the first to set many milestones while Reid's election and re-ellection has arguably been in smoother conditions. Moseley-Braun is an African-American woman to be the first female U.S. Senator in her state while Reid is from an already well represented identity group - white and male.\n",
    "\n",
    "The similarity being above 50% might be due to the fact that they are from the same party but the present difference is, at least superficially, because they are vastly different character and from very different states.\n",
    "\n",
    "For a better grounded analysis, we can look at the similarity measures of multiple pairs of senator speeches from the same and different parties and employ a comparative perspective. This approach can reveal patterns more clearly as to what makes two speeches similar and what having similar speeches tells us about the characteristics of the senators in comparison. Furthermore, different methods of vectorizing speeches and different measures of similarity might give qualitatively different results.\n",
    "\n",
    "For instance, we found here a similarity of 65%. A good reference point would be the average level of similarity between senators of the two different parties.\n",
    "\n",
    "##### What about when we use a different vectorizer and look at different n-gram ranges?\n",
    "\n",
    "- **Observation 1**: We see that for every n-gram range, Bag-of-Words gives a higher cosine similarity. This is because TF-IDF is more restrictive. While Bag-of-Words simply records how many times each word is used in both texts, TF-IDF (Term Frequency-Inverse Document Frequency) gives a measure of how often word i (or n-gram i) appeared in text j, penalized by the number of texts also containing word i. In other words, the weight of a word is proportional to its frequency in the document (term frequency) and inversely proportional to its frequency across the corpus (inverse document frequency). Words that are common across the corpus (i.e., appear in many documents) receive a lower weight, while words that are rare in the corpus receive a higher weight.\n",
    "\n",
    "- **Observation 2**: We look n-grams alone and cumulatively i.e., 1, 2 and 3-grams together for (1,3) n-gram range. If we take the first case, looking at 2-grams or 3-grams alone, we see for both vectorizers the cosine similarities strictly decrease as we look at larger grams. In the cumulative case, we see again cosine similarities decreasing. However, the decrease in this case is slower because it is easier to get a high measure of cosine similarity when looking at 1-grams than 2-grams and easier when looking at 2-grams than 3-grams. Looking at a range of 1 to 3 grams rather than just 3 grams results in higher cosine similarity due to the above logic.\n",
    "\n",
    "##### What about a different similarity measure?\n",
    "We also look at *Manhattan Distance* measures between text vectors. Manhattan Distance is the distance between two vectors as the sum of the absolute differences between the elements of the two vectors. It is also known as L1 distance. A lower value indicates greater similarity. See [Taxicab Geometry](https://en.wikipedia.org/wiki/Taxicab_geometry).\n",
    "\n",
    "Since it is a distance measure, the lower the value, the more similar the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Similarity Measure</th>\n",
       "      <th>Ngram Range</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>16.875941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>61.226410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>108.874310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>155.732114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>122.857987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>169.456308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>213.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>64225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>217514.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>392813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>571048.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>153289.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>175299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bag-of-Words</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>178235.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Vectorizer Similarity Measure Ngram Range  Similarity Score\n",
       "1          Tfidf          Manhattan      (1, 1)         16.875941\n",
       "3          Tfidf          Manhattan      (1, 2)         61.226410\n",
       "5          Tfidf          Manhattan      (1, 3)        108.874310\n",
       "7          Tfidf          Manhattan      (1, 4)        155.732114\n",
       "9          Tfidf          Manhattan      (2, 2)        122.857987\n",
       "11         Tfidf          Manhattan      (3, 3)        169.456308\n",
       "13         Tfidf          Manhattan      (4, 4)        213.775900\n",
       "15  Bag-of-Words          Manhattan      (1, 1)      64225.000000\n",
       "17  Bag-of-Words          Manhattan      (1, 2)     217514.000000\n",
       "19  Bag-of-Words          Manhattan      (1, 3)     392813.000000\n",
       "21  Bag-of-Words          Manhattan      (1, 4)     571048.000000\n",
       "23  Bag-of-Words          Manhattan      (2, 2)     153289.000000\n",
       "25  Bag-of-Words          Manhattan      (3, 3)     175299.000000\n",
       "27  Bag-of-Words          Manhattan      (4, 4)     178235.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Similarity Measure\"] == \"Manhattan\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with Manhattan Distance as the similarity measure mirrors the results when we used cosine similarity in terms of comparing within the same vectorizer i.e., with a given vectorizer, the observation about using different n-gram ranges hold here.\n",
    "\n",
    "But since the [*Manhattan Distance*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html#sklearn.metrics.pairwise.manhattan_distances) measure in **sci-kit learn** library is not a standardized measure like *Cosine Similarity*, we cannot compare along the results from different vectorizers with the same n-gram range.\n",
    "\n",
    "For instance, if we look at the the Manhattan Score using Bag-of-Words with 1-grams and the corresponding score using TF-IDF, we cannot infer that there is a dissimilarity of the order of thousands between the two scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What else can be done in future projects?\n",
    "\n",
    "- Corpus specific useless words can be eliminated during the text processing stage.\n",
    "- More steps can be written as functions to avoid unnecessary code repetition.\n",
    "- A pipeline can be constructed in order to check among N texts, which one is the most similar to a given text. For example, among all the senator speeches we have, which one is the most similar to a given senator's e.g., Senator Biden's."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us now look at among the speech texts we have which one is the most similar to Senator Biden's.\n",
    "We use;\n",
    "- TF-IDF to vectorize,\n",
    "- Cosine similarity to compare similarities, and,\n",
    "- Cumulative 2-grams i.e., 1-grams and 2-grams together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the URLs that contain the text files:\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "html = requests.get('https://github.com/ariedamuco/ML-for-NLP/tree/main/Inputs/105-extracted-date')\n",
    "\n",
    "text_links = []\n",
    "\n",
    "# Putting links to each text file into a list:\n",
    "for link in BeautifulSoup(html.text, parse_only=SoupStrainer('a')):\n",
    "    if hasattr(link, 'href') and link['href'].endswith('.txt'):\n",
    "        url = \"https://raw.githubusercontent.com\" + link['href'].replace('/blob/', '/')\n",
    "        text_links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all the texts into a dictionary:\n",
    "\n",
    "text_dict = {}\n",
    "\n",
    "for i, text_url in enumerate(text_links):\n",
    "    text_get = requests.get(text_url)\n",
    "    text = text_get.text\n",
    "    \n",
    "    key = 'text{}'.format(i+1)\n",
    "    text_dict[key] = text\n",
    "\n",
    "# The key for Senator Biden is 'text7'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text3', 'text2', 'text75', 'text14', 'text54', 'text23', 'text17', 'text31', 'text27', 'text81', 'text49', 'text33', 'text89', 'text5', 'text38', 'text28', 'text70', 'text6', 'text18', 'text20', 'text7'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a random subsample of files because otherwise it takes too much time to process.\n",
    "\n",
    "import random\n",
    "\n",
    "text7 = text_dict.pop('text7')\n",
    "\n",
    "keys = list(text_dict.keys())\n",
    "\n",
    "random_keys = random.sample(keys, 20)\n",
    "\n",
    "#random_keys.append('text7')\n",
    "\n",
    "random_text_dict = {key: text_dict[key] for key in random_keys}\n",
    "\n",
    "random_text_dict['text7'] = text7\n",
    "\n",
    "random_text_dict.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Since the subsample is selected randomly, the code will produce a different subsample at each run. When I ran the code myself for the first time I found speeches of Senator Lieberman as the most and Senator Helms as the least similar. Hence, the discussion in the end is based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing processed texts in a dictionary:\n",
    "\n",
    "processed_text_dict = {}\n",
    "\n",
    "for key, text in random_text_dict.items():\n",
    "    processed_text = text_processor(text)\n",
    "    \n",
    "    processed_text_dict[key] = processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 biden 19981021 docno text biden presid plea senat today pas hatch biden lautenberg sub'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-checking if text7 indeed belongs to Senator Biden:\n",
    "processed_text_dict['text7'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the subsample of texts:\n",
    "corpus = tfidf_vectorizer_12.fit_transform(processed_text_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_matrix = pd.DataFrame(corpus.toarray().transpose(), \n",
    "                             index=tfidf_vectorizer_12.get_feature_names_out())\n",
    "\n",
    "# Renaming the columns with the textID format i.e., their keys in the dictionary:\n",
    "corpus_matrix = corpus_matrix.set_axis(list(processed_text_dict.keys()), \n",
    "                                       axis = \"columns\", \n",
    "                                       copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text3</th>\n",
       "      <th>text2</th>\n",
       "      <th>text75</th>\n",
       "      <th>text14</th>\n",
       "      <th>text54</th>\n",
       "      <th>text23</th>\n",
       "      <th>text17</th>\n",
       "      <th>text31</th>\n",
       "      <th>text27</th>\n",
       "      <th>text81</th>\n",
       "      <th>...</th>\n",
       "      <th>text33</th>\n",
       "      <th>text89</th>\n",
       "      <th>text5</th>\n",
       "      <th>text38</th>\n",
       "      <th>text28</th>\n",
       "      <th>text70</th>\n",
       "      <th>text6</th>\n",
       "      <th>text18</th>\n",
       "      <th>text20</th>\n",
       "      <th>text7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "      <td>779490.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.001129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.375865</td>\n",
       "      <td>0.251646</td>\n",
       "      <td>0.358107</td>\n",
       "      <td>0.375152</td>\n",
       "      <td>0.330256</td>\n",
       "      <td>0.291310</td>\n",
       "      <td>0.398921</td>\n",
       "      <td>0.277187</td>\n",
       "      <td>0.372142</td>\n",
       "      <td>0.311811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293204</td>\n",
       "      <td>0.312407</td>\n",
       "      <td>0.356939</td>\n",
       "      <td>0.226659</td>\n",
       "      <td>0.263023</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>0.364171</td>\n",
       "      <td>0.416258</td>\n",
       "      <td>0.355191</td>\n",
       "      <td>0.326052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               text3          text2         text75         text14  \\\n",
       "count  779490.000000  779490.000000  779490.000000  779490.000000   \n",
       "mean        0.000071       0.000097       0.000092       0.000067   \n",
       "std         0.001130       0.001129       0.001129       0.001131   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         0.375865       0.251646       0.358107       0.375152   \n",
       "\n",
       "              text54         text23         text17         text31  \\\n",
       "count  779490.000000  779490.000000  779490.000000  779490.000000   \n",
       "mean        0.000078       0.000084       0.000053       0.000089   \n",
       "std         0.001130       0.001129       0.001131       0.001129   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         0.330256       0.291310       0.398921       0.277187   \n",
       "\n",
       "              text27         text81  ...         text33         text89  \\\n",
       "count  779490.000000  779490.000000  ...  779490.000000  779490.000000   \n",
       "mean        0.000072       0.000083  ...       0.000082       0.000081   \n",
       "std         0.001130       0.001130  ...       0.001130       0.001130   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "75%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "max         0.372142       0.311811  ...       0.293204       0.312407   \n",
       "\n",
       "               text5         text38         text28         text70  \\\n",
       "count  779490.000000  779490.000000  779490.000000  779490.000000   \n",
       "mean        0.000070       0.000103       0.000094       0.000076   \n",
       "std         0.001130       0.001128       0.001129       0.001130   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         0.356939       0.226659       0.263023       0.352307   \n",
       "\n",
       "               text6         text18         text20          text7  \n",
       "count  779490.000000  779490.000000  779490.000000  779490.000000  \n",
       "mean        0.000076       0.000053       0.000079       0.000090  \n",
       "std         0.001130       0.001131       0.001130       0.001129  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000       0.000000  \n",
       "max         0.364171       0.416258       0.355191       0.326052  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each text into their own vectors:\n",
    "for i, col_name in enumerate(list(corpus_matrix.columns)):\n",
    "    globals()[\"TFIDF_\" + str(col_name)] =corpus_matrix[corpus_matrix.columns[i]].values.reshape(1, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01359197, 0.00041845, 0.00044001, ..., 0.00118421, 0.00039474,\n",
       "        0.00039474]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to contain pairwise similarities of speeches to Senator Biden's:\n",
    "cosine_similarities_dict = {'Cosine Similarity': 'NaN', 'Text': (list(corpus_matrix.columns))}\n",
    "cosine_similarities = pd.DataFrame(data=cosine_similarities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cosine Similarity    Text\n",
       "0                NaN   text3\n",
       "1                NaN   text2\n",
       "2                NaN  text75\n",
       "3                NaN  text14\n",
       "4                NaN  text54\n",
       "5                NaN  text23\n",
       "6                NaN  text17\n",
       "7                NaN  text31\n",
       "8                NaN  text27\n",
       "9                NaN  text81\n",
       "10               NaN  text49\n",
       "11               NaN  text33\n",
       "12               NaN  text89\n",
       "13               NaN   text5\n",
       "14               NaN  text38\n",
       "15               NaN  text28\n",
       "16               NaN  text70\n",
       "17               NaN   text6\n",
       "18               NaN  text18\n",
       "19               NaN  text20\n",
       "20               NaN   text7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cosine similarities and writing them to the dataframe:\n",
    "for i, col_name in enumerate(list(corpus_matrix.columns)):\n",
    "    cosine_similarities['Cosine Similarity'][i] = cosine_similarity(TFIDF_text7, globals()[\"TFIDF_\" + str(col_name)])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Biden's speech:\n",
    "cosine_similarities = cosine_similarities.drop(index=cosine_similarities[cosine_similarities['Text'] == 'text7'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.487042</td>\n",
       "      <td>text3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.436158</td>\n",
       "      <td>text2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.538425</td>\n",
       "      <td>text75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.517219</td>\n",
       "      <td>text14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.481797</td>\n",
       "      <td>text54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.539758</td>\n",
       "      <td>text23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.438911</td>\n",
       "      <td>text17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.564684</td>\n",
       "      <td>text31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.53823</td>\n",
       "      <td>text27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.515049</td>\n",
       "      <td>text81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.329111</td>\n",
       "      <td>text49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.535202</td>\n",
       "      <td>text33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.517461</td>\n",
       "      <td>text89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.508802</td>\n",
       "      <td>text5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.512829</td>\n",
       "      <td>text38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.569635</td>\n",
       "      <td>text28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.503465</td>\n",
       "      <td>text70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.515753</td>\n",
       "      <td>text6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.459363</td>\n",
       "      <td>text18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.544545</td>\n",
       "      <td>text20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cosine Similarity    Text\n",
       "0           0.487042   text3\n",
       "1           0.436158   text2\n",
       "2           0.538425  text75\n",
       "3           0.517219  text14\n",
       "4           0.481797  text54\n",
       "5           0.539758  text23\n",
       "6           0.438911  text17\n",
       "7           0.564684  text31\n",
       "8            0.53823  text27\n",
       "9           0.515049  text81\n",
       "10          0.329111  text49\n",
       "11          0.535202  text33\n",
       "12          0.517461  text89\n",
       "13          0.508802   text5\n",
       "14          0.512829  text38\n",
       "15          0.569635  text28\n",
       "16          0.503465  text70\n",
       "17          0.515753   text6\n",
       "18          0.459363  text18\n",
       "19          0.544545  text20"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking who has the closest speech:\n",
    "\n",
    "cosine_similarities['Cosine Similarity'] = cosine_similarities['Cosine Similarity'].astype(float)\n",
    "max_index = cosine_similarities['Cosine Similarity'].idxmax()\n",
    "\n",
    "max_text = cosine_similarities.loc[max_index]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text28'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 dewin 19981021 docno text dewin presid 1023 ricki ray hemophilia relief fund act would author establish fund compassion payment would made peopl hemophilia contract hiv aid taint blood product earli 1980 peopl victim failur feder govern safeguard blood product failur includ inadequ measur screen high risk donor long delay recal blood product known pose elev risk infect time period specifi legisl approxim 200 victim infect victim victim famili would receiv singl 100 000 payment total author 750 000 would separ appropri relief fund sunset year 1023 pas hous without object suspens calendar may similar legisl senat 358 sponsor bipartisan cosponsor text doc doc docno 105 dewin 19981021 docno text dewin presid ask unanim consent order quorum call rescind text doc doc docno 105 dewin 19981021 docno text dewin presid first let thank major leader passag ricki ray bill occur moment ago bill introduc along senat bob graham senat introduc hous repres repres tauzin certainli work help'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text_dict[max_text][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://raw.githubusercontent.com/ariedamuco/ML-for-NLP/main/Inputs/105-extracted-date/105-dewine-oh.txt'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_links[int(max_text[-2:])-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Speech Most Similar to Senator Biden's\n",
    "\n",
    "The most similar speech to Biden's in the random subset of texts seems to be Senator [Joe Lieberman](https://en.wikipedia.org/wiki/Joe_Lieberman).\n",
    "\n",
    "It is plausible because:\n",
    "- Although he went independent in 2006, he has been a **Democrat** since the beginning of his political career.\n",
    "- He, before the 2016 election, he endorsed Hillary Clinton for president and in 2020 endorsed **Joe Biden** for president.\n",
    "- Lieberman says about Joe Biden in [this](https://www.theguardian.com/us-news/2021/nov/24/joe-lieberman-most-republicans-democrats-centrists) article on The Guardian: “Biden is solid. He sees the world realistically and he knows he can’t be Roosevelt or Lyndon Johnson now in part because he doesn’t have the great Democratic majorities that they had.”\n",
    "\n",
    "“And the country, thank God, is not where it was in the Depression, as bad as the pandemic was. The old Joe, which is the real Joe, will be dominant in the next three years of his presidency.”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us finally look at the least similar speech in the subsample as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cosine Similarity    0.329111\n",
       "Text                   text49\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_index = cosine_similarities['Cosine Similarity'].idxmin()\n",
    "cosine_similarities.loc[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_text = cosine_similarities.loc[min_index]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc docno 105 helm 19981021 docno text helm presid follow senat approv resolut ratif chemic weapon convent cwc subsequ ratif treati presid becam necessari unit state enact legisl implement variou domest oblig foreign relat judiciari committe senat immedi fulfil oblig prepar implement legisl treati ratifi may 1997 full senat pas 610 chemic weapon convent implement act 1997 soon thereaft novemb 1997 hous repres pas implement legisl togeth sanction russian firm assist iran ballist missil program regret taken long enact implement legisl law reason expect numer compani challeng constitution treati overturn court unfortun final resolut legal issu surround cwc well full complianc treati delay entir session congress presid clinton opposit unrel missil sanction provis bill inde presid sought delay derail cwc implement legisl throughout entir spring presid alon respons put unit state noncompli delay ultim veto bill june 1998 import frustrat slow pace implement cwc understand congress discharg ob'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text_dict[min_text][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://raw.githubusercontent.com/ariedamuco/ML-for-NLP/main/Inputs/105-extracted-date/105-helms-nc.txt'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_links[int(min_text[-2:])-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the speech of Senator Jesse Helms of North Carolina.\n",
    "\n",
    "A quick look at the [Wikipedia page](https://en.wikipedia.org/wiki/Jesse_Helms) on him shows that:\n",
    "- He is a leader in the conservative movement.\n",
    "- A Republican since 1970 to 2008 (previously a Democrat for a short time), which is when he passed.\n",
    "- Wikipedia article reads: \"[He] opposed civil rights, disability rights, environmentalism, feminism, gay rights, affirmative action, access to abortions, the Religious Freedom Restoration Act (RFRA), and the National Endowment for the Arts. Helms brought an \"aggressiveness\" to his conservatism, as in his rhetoric against homosexuality.\"\n",
    "\n",
    "At least superficially, he seems to have opposed most of the things Biden stands for today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd99e0ac365466ae650ff45d8775d59e259e5553338a3c6426640fcc04918b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
